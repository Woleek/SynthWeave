<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.5">
<title>synthweave.loss.embedding API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>synthweave.loss.embedding</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="synthweave.loss.embedding.ContrastiveLoss"><code class="flex name class">
<span>class <span class="ident">ContrastiveLoss</span></span>
<span>(</span><span>margin: float = 0.2)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ContrastiveLoss(nn.Module):
    &#34;&#34;&#34;Contrastive loss for learning discriminative embeddings.

    Minimizes the distance between similar pairs and maximizes the distance between
    dissimilar pairs up to a margin. This helps create embeddings where similar
    samples are close together and dissimilar samples are far apart.

    Based on: &#34;Dimensionality Reduction by Learning an Invariant Mapping&#34;
    Source: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf

    Attributes:
        margin: Minimum distance margin between dissimilar pairs
    &#34;&#34;&#34;

    def __init__(self, margin: float = 0.2):
        &#34;&#34;&#34;Initialize the contrastive loss module.

        Args:
            margin: Minimum distance margin between dissimilar pairs
        &#34;&#34;&#34;
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Compute the contrastive loss between a set of embeddings.

        Args:
            embeddings: Feature embeddings of shape (batch_size, embed_dim)
            labels: Ground truth labels of shape (batch_size,)

        Returns:
            torch.Tensor: Scalar loss value

        Process:
            1. Compute pairwise Euclidean distances between embeddings
            2. Create label match matrix to identify similar/dissimilar pairs
            3. Compute loss for positive pairs (similar labels)
            4. Compute loss for negative pairs (different labels)
            5. Combine and normalize the losses
        &#34;&#34;&#34;
        # Compute pairwise distances
        distances = (
            torch.cdist(embeddings, embeddings, p=2) + 1e-8
        )  # Shape: (batch_size, batch_size)

        # Create label match matrix
        label_matrix = (
            labels.unsqueeze(0) == labels.unsqueeze(1)
        ).float()  # Shape: (batch_size, batch_size)

        # Positive pairs: same labels
        pos_loss = (label_matrix) * (distances**2)

        # Negative pairs: different labels
        neg_loss = (1 - label_matrix) * torch.clamp(self.margin - distances, min=0) ** 2

        # Compute loss
        loss = (pos_loss + neg_loss).sum() / embeddings.size(0)

        return loss</code></pre>
</details>
<div class="desc"><p>Contrastive loss for learning discriminative embeddings.</p>
<p>Minimizes the distance between similar pairs and maximizes the distance between
dissimilar pairs up to a margin. This helps create embeddings where similar
samples are close together and dissimilar samples are far apart.</p>
<p>Based on: "Dimensionality Reduction by Learning an Invariant Mapping"
Source: <a href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf">http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf</a></p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>margin</code></strong></dt>
<dd>Minimum distance margin between dissimilar pairs</dd>
</dl>
<p>Initialize the contrastive loss module.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>margin</code></strong></dt>
<dd>Minimum distance margin between dissimilar pairs</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="synthweave.loss.embedding.ContrastiveLoss.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, embeddings: torch.Tensor, labels: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Compute the contrastive loss between a set of embeddings.

    Args:
        embeddings: Feature embeddings of shape (batch_size, embed_dim)
        labels: Ground truth labels of shape (batch_size,)

    Returns:
        torch.Tensor: Scalar loss value

    Process:
        1. Compute pairwise Euclidean distances between embeddings
        2. Create label match matrix to identify similar/dissimilar pairs
        3. Compute loss for positive pairs (similar labels)
        4. Compute loss for negative pairs (different labels)
        5. Combine and normalize the losses
    &#34;&#34;&#34;
    # Compute pairwise distances
    distances = (
        torch.cdist(embeddings, embeddings, p=2) + 1e-8
    )  # Shape: (batch_size, batch_size)

    # Create label match matrix
    label_matrix = (
        labels.unsqueeze(0) == labels.unsqueeze(1)
    ).float()  # Shape: (batch_size, batch_size)

    # Positive pairs: same labels
    pos_loss = (label_matrix) * (distances**2)

    # Negative pairs: different labels
    neg_loss = (1 - label_matrix) * torch.clamp(self.margin - distances, min=0) ** 2

    # Compute loss
    loss = (pos_loss + neg_loss).sum() / embeddings.size(0)

    return loss</code></pre>
</details>
<div class="desc"><p>Compute the contrastive loss between a set of embeddings.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embeddings</code></strong></dt>
<dd>Feature embeddings of shape (batch_size, embed_dim)</dd>
<dt><strong><code>labels</code></strong></dt>
<dd>Ground truth labels of shape (batch_size,)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Scalar loss value</dd>
</dl>
<h2 id="process">Process</h2>
<ol>
<li>Compute pairwise Euclidean distances between embeddings</li>
<li>Create label match matrix to identify similar/dissimilar pairs</li>
<li>Compute loss for positive pairs (similar labels)</li>
<li>Compute loss for negative pairs (different labels)</li>
<li>Combine and normalize the losses</li>
</ol></div>
</dd>
</dl>
</dd>
<dt id="synthweave.loss.embedding.ContrastiveLossWithBatchBalancing"><code class="flex name class">
<span>class <span class="ident">ContrastiveLossWithBatchBalancing</span></span>
<span>(</span><span>margin: float = 0.2)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ContrastiveLossWithBatchBalancing(nn.Module):
    &#34;&#34;&#34;Contrastive loss with batch-wise pair balancing.

    An enhanced version of ContrastiveLoss that normalizes the loss by the number
    of valid pairs in each category (similar/dissimilar). This helps address class
    imbalance issues and provides more stable training.

    Attributes:
        margin: Minimum distance margin between dissimilar pairs
    &#34;&#34;&#34;

    def __init__(self, margin: float = 0.2):
        &#34;&#34;&#34;Initialize the balanced contrastive loss module.

        Args:
            margin: Minimum distance margin between dissimilar pairs
        &#34;&#34;&#34;
        super(ContrastiveLossWithBatchBalancing, self).__init__()
        self.margin = margin

    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Compute the balanced contrastive loss between a set of embeddings.

        Args:
            embeddings: Feature embeddings of shape (batch_size, embed_dim)
            labels: Ground truth labels of shape (batch_size,)

        Returns:
            torch.Tensor: Scalar loss value

        Process:
            1. Compute pairwise Euclidean distances between embeddings
            2. Create label match matrix to identify similar/dissimilar pairs
            3. Compute loss for positive pairs (similar labels)
            4. Compute loss for negative pairs (different labels)
            5. Normalize each loss term by its number of pairs
            6. Sum the normalized losses
        &#34;&#34;&#34;
        # Compute pairwise distances
        distances = (
            torch.cdist(embeddings, embeddings, p=2) + 1e-8
        )  # Shape: (batch_size, batch_size)

        # Create label match matrix
        label_matrix = (
            labels.unsqueeze(0) == labels.unsqueeze(1)
        ).float()  # Shape: (batch_size, batch_size)

        # Positive pairs: same labels
        pos_loss = (label_matrix) * (distances**2)

        # Negative pairs: different labels
        neg_loss = (1 - label_matrix) * torch.clamp(self.margin - distances, min=0) ** 2

        # Normalize by the number of valid pairs
        num_pos_pairs = label_matrix.sum()
        num_neg_pairs = (1 - label_matrix).sum()

        # Compute loss
        pos_loss = pos_loss.sum() / (num_pos_pairs + 1e-8)
        neg_loss = neg_loss.sum() / (num_neg_pairs + 1e-8)
        loss = pos_loss + neg_loss

        return loss</code></pre>
</details>
<div class="desc"><p>Contrastive loss with batch-wise pair balancing.</p>
<p>An enhanced version of ContrastiveLoss that normalizes the loss by the number
of valid pairs in each category (similar/dissimilar). This helps address class
imbalance issues and provides more stable training.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>margin</code></strong></dt>
<dd>Minimum distance margin between dissimilar pairs</dd>
</dl>
<p>Initialize the balanced contrastive loss module.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>margin</code></strong></dt>
<dd>Minimum distance margin between dissimilar pairs</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="synthweave.loss.embedding.ContrastiveLossWithBatchBalancing.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, embeddings: torch.Tensor, labels: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Compute the balanced contrastive loss between a set of embeddings.

    Args:
        embeddings: Feature embeddings of shape (batch_size, embed_dim)
        labels: Ground truth labels of shape (batch_size,)

    Returns:
        torch.Tensor: Scalar loss value

    Process:
        1. Compute pairwise Euclidean distances between embeddings
        2. Create label match matrix to identify similar/dissimilar pairs
        3. Compute loss for positive pairs (similar labels)
        4. Compute loss for negative pairs (different labels)
        5. Normalize each loss term by its number of pairs
        6. Sum the normalized losses
    &#34;&#34;&#34;
    # Compute pairwise distances
    distances = (
        torch.cdist(embeddings, embeddings, p=2) + 1e-8
    )  # Shape: (batch_size, batch_size)

    # Create label match matrix
    label_matrix = (
        labels.unsqueeze(0) == labels.unsqueeze(1)
    ).float()  # Shape: (batch_size, batch_size)

    # Positive pairs: same labels
    pos_loss = (label_matrix) * (distances**2)

    # Negative pairs: different labels
    neg_loss = (1 - label_matrix) * torch.clamp(self.margin - distances, min=0) ** 2

    # Normalize by the number of valid pairs
    num_pos_pairs = label_matrix.sum()
    num_neg_pairs = (1 - label_matrix).sum()

    # Compute loss
    pos_loss = pos_loss.sum() / (num_pos_pairs + 1e-8)
    neg_loss = neg_loss.sum() / (num_neg_pairs + 1e-8)
    loss = pos_loss + neg_loss

    return loss</code></pre>
</details>
<div class="desc"><p>Compute the balanced contrastive loss between a set of embeddings.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embeddings</code></strong></dt>
<dd>Feature embeddings of shape (batch_size, embed_dim)</dd>
<dt><strong><code>labels</code></strong></dt>
<dd>Ground truth labels of shape (batch_size,)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Scalar loss value</dd>
</dl>
<h2 id="process">Process</h2>
<ol>
<li>Compute pairwise Euclidean distances between embeddings</li>
<li>Create label match matrix to identify similar/dissimilar pairs</li>
<li>Compute loss for positive pairs (similar labels)</li>
<li>Compute loss for negative pairs (different labels)</li>
<li>Normalize each loss term by its number of pairs</li>
<li>Sum the normalized losses</li>
</ol></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="synthweave.loss" href="index.html">synthweave.loss</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="synthweave.loss.embedding.ContrastiveLoss" href="#synthweave.loss.embedding.ContrastiveLoss">ContrastiveLoss</a></code></h4>
<ul class="">
<li><code><a title="synthweave.loss.embedding.ContrastiveLoss.forward" href="#synthweave.loss.embedding.ContrastiveLoss.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="synthweave.loss.embedding.ContrastiveLossWithBatchBalancing" href="#synthweave.loss.embedding.ContrastiveLossWithBatchBalancing">ContrastiveLossWithBatchBalancing</a></code></h4>
<ul class="">
<li><code><a title="synthweave.loss.embedding.ContrastiveLossWithBatchBalancing.forward" href="#synthweave.loss.embedding.ContrastiveLossWithBatchBalancing.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.5</a>.</p>
</footer>
</body>
</html>
