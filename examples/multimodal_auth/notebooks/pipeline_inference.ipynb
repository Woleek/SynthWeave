{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.pipe import MultiModalAuthPipeline, ImagePreprocessor, AudioPreprocessor, AdaFace, ReDimNet\n",
    "from synthweave.utils.datasets import get_datamodule, SWAN_DF_Dataset\n",
    "from synthweave.utils.fusion import get_fusion\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_kwargs = {\n",
    "    \"root_real\": \"/home/woleek/SynthWeave/data/SWAN-Idiap\",\n",
    "    \"root_df\": \"/home/woleek/SynthWeave/data/SWAN-DF\",\n",
    "    \"resolutions\": [\"320x320\"]\n",
    "}\n",
    "\n",
    "dm = get_datamodule(\n",
    "    \"SWAN_DF\",\n",
    "    dataset_cls=SWAN_DF_Dataset,\n",
    "    batch_size=1, # NOTE: currently single window fusions don't ignore padding\n",
    "    dataset_kwargs=ds_kwargs,\n",
    "    sample_mode=\"sequence\",  # single, sequence\n",
    "    clip_mode = None,\n",
    "    pad_mode = 'zeros'\n",
    ")\n",
    "\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = dm.test_dataloader()\n",
    "next(iter(test_loader))['video'].squeeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_module_dir = Path(\"/home/woleek/SynthWeave/SynthWeave/examples/multimodal_auth/CAFF\")\n",
    "\n",
    "# config\n",
    "args = json.loads((fusion_module_dir / \"args.json\").read_text())\n",
    "args = dotdict(args)\n",
    "\n",
    "# weights\n",
    "weights_path = fusion_module_dir / \"detection_module.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = {\n",
    "    \"video\": ImagePreprocessor(\n",
    "        window_len=4,\n",
    "        step=1,\n",
    "        estimate_quality=True,\n",
    "        models_dir=\"/home/woleek/SynthWeave/models\",\n",
    "    ),\n",
    "    \"audio\": AudioPreprocessor(\n",
    "        window_len=4,\n",
    "        step=1,\n",
    "        use_vad=True,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"audio\": AdaFace(\n",
    "        path=\"/home/woleek/SynthWeave/models\"\n",
    "    ), \n",
    "    \"video\": ReDimNet()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion = get_fusion(\n",
    "    fusion_name=args.fusion,\n",
    "    output_dim=args.emb_dim,\n",
    "    modality_keys=[\"video\", \"audio\"],\n",
    "    input_dims={\"video\":512, \"audio\":192},\n",
    "    out_proj_dim=args.proj_dim,\n",
    ")\n",
    "\n",
    "detection_head = torch.nn.ModuleDict([\n",
    "    (\"classifier\", torch.nn.Sequential(\n",
    "        torch.nn.Linear(args.emb_dim, 1)\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = MultiModalAuthPipeline(\n",
    "    processors=preprocessors,\n",
    "    models=models,\n",
    "    fusion=fusion,\n",
    "    detection_head=detection_head,\n",
    "    freeze_backbone=True,\n",
    "    iil_mode=args.iil_mode,\n",
    ")\n",
    "\n",
    "state_dict = torch.load(weights_path, map_location=\"cpu\")['state_dict']\n",
    "state_dict = {k.replace(\"pipeline.\", \"\"): v for k, v in state_dict.items()}\n",
    "pipe.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "pipe = pipe.cuda()\n",
    "pipe.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(test_loader))\n",
    "\n",
    "# Place on GPU\n",
    "sample[\"video\"] = sample[\"video\"].squeeze(0).cuda()\n",
    "sample[\"audio\"] = sample[\"audio\"].squeeze(0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = pipe(sample)\n",
    "    \n",
    "    preds = (out[\"logits\"] > 0.5).type(torch.int64).cpu()\n",
    "    final_pred = torch.mode(preds, dim=0).values # NOTE: Majority vote\n",
    "    \n",
    "    print(\"GT:\", sample['metadata'][\"label\"].cpu().item())\n",
    "    print(\"Pred:\", final_pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"video_ref\"] = torch.rand_like(out[\"video\"])\n",
    "out[\"audio_ref\"] = torch.rand_like(out[\"audio\"])\n",
    "\n",
    "# out['video_ref'] = out['video'].clone()\n",
    "# out['audio_ref'] = out['audio'].clone()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sim = pipe.verify(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim[\"video\"][0].item(), sim[\"audio\"][0].item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
