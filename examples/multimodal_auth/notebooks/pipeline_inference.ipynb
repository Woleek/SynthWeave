{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(__file__).resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.pipe import MultiModalAuthPipeline, ImagePreprocessor, AudioPreprocessor, AdaFace, ReDimNet\n",
    "from synthweave.utils.datasets import get_datamodule\n",
    "from synthweave.utils.fusion import get_fusion\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from torchmetrics.classification import F1Score, Accuracy\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_kwargs = {\n",
    "    \"data_dir\": \"../encoded_data/DeepSpeak_v1_1\",\n",
    "    \"preprocessed\": True,\n",
    "    \"sample_mode\": \"sequence\",\n",
    "}\n",
    "\n",
    "dm = get_datamodule(\n",
    "    \"DeepSpeak_v1_1\",\n",
    "    batch_size=1, # NOTE: currently single window fusions don't ignore padding\n",
    "    dataset_kwargs=ds_kwargs,\n",
    "    sample_mode=\"sequence\",  # single, sequence\n",
    "    clip_mode = None,\n",
    "    pad_mode = 'zeros'\n",
    ")\n",
    "\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = dm.test_dataloader()\n",
    "next(iter(test_loader))['video'].squeeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUSION = \"MMD\"\n",
    "TASK = \"binary\"\n",
    "\n",
    "path = Path(\"logs\") / TASK / FUSION\n",
    "path = sorted(path.glob(\"version_*\"))[-1]\n",
    "\n",
    "# config\n",
    "args = json.loads((path / \"args.json\").read_text())\n",
    "args = dotdict(args)\n",
    "\n",
    "# best checkpoint\n",
    "ckpt = path / \"checkpoints\"\n",
    "ckpt = sorted(ckpt.glob(\"epoch=*.ckpt\"))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion = get_fusion(\n",
    "    fusion_name=FUSION,\n",
    "    output_dim=512,\n",
    "    modality_keys=[\"video\", \"audio\"],\n",
    "    out_proj_dim=1024,\n",
    "    num_att_heads=4,  # only for attention-based fusions\n",
    "    n_layers=3,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "fusion.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/SynthWeave/SynthWeave/synthweave/fusion/base.py:160\u001b[0m, in \u001b[0;36mBaseFusion.__call__\u001b[0;34m(self, embeddings, output_projections)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m, embeddings: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor], output_projections: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    159\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_projections\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SynthWeave/SynthWeave/synthweave/fusion/base.py:213\u001b[0m, in \u001b[0;36mBaseFusion.forward\u001b[0;34m(self, embeddings, output_projections)\u001b[0m\n\u001b[1;32m    210\u001b[0m         proj_embeddings[mod] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection[mod](embed)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Perform fusion\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m fused_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Refine the fused embedding\u001b[39;00m\n\u001b[1;32m    216\u001b[0m fused_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefiner(fused_embedding)\n",
      "File \u001b[0;32m~/SynthWeave/SynthWeave/synthweave/fusion/mmd/mmd.py:110\u001b[0m, in \u001b[0;36mMMD._forward\u001b[0;34m(self, embeddings)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Unsqueeze T dim\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# embeddings = [emb.unsqueeze(1) for emb in embeddings] # list[(B,1,PROJ)]\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Pass through L MMDBlocks\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 110\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# list length preserved\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Concatenate the refined modality features\u001b[39;00m\n\u001b[1;32m    113\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m [emb[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m embeddings] \u001b[38;5;66;03m# list[(B, PROJ)]\u001b[39;00m\n",
      "File \u001b[0;32m~/SynthWeave/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SynthWeave/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/SynthWeave/SynthWeave/synthweave/fusion/mmd/modules.py:210\u001b[0m, in \u001b[0;36mMMDBlock.forward\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass for a single MMDBlock.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    3. Apply FeedForward with residual connection and layer norm\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# 1) cross-modal\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m cross_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbi_cro_att\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# list[(B,T,d)]\u001b[39;00m\n\u001b[1;32m    211\u001b[0m x1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(t \u001b[38;5;241m+\u001b[39m c) \u001b[38;5;28;01mfor\u001b[39;00m t, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, cross_outs)]\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# 2) self-att per modality\u001b[39;00m\n",
      "File \u001b[0;32m~/SynthWeave/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SynthWeave/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/SynthWeave/SynthWeave/synthweave/fusion/mmd/modules.py:147\u001b[0m, in \u001b[0;36mBiCroAttention.forward\u001b[0;34m(self, P)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, P: \u001b[38;5;28mlist\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute bi-directional cross-attention for the i-th modality.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        3. Concatenate attended features\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     B, T, d \u001b[38;5;241m=\u001b[39m \u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    148\u001b[0m     outs: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, Pi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(P):\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "fusion({\"audio\": torch.randn(1, 128), \"video\": torch.randn(1, 512)}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] This fusion expects embeddings of shape (batch_size, embed_dim).\n"
     ]
    }
   ],
   "source": [
    "models = {\"audio\": torch.nn.Identity(), \"video\": torch.nn.Identity()}\n",
    "\n",
    "EMB_DIM = args.emb_dim\n",
    "\n",
    "fusion = get_fusion(\n",
    "    fusion_name=FUSION,\n",
    "    output_dim=EMB_DIM,\n",
    "    modality_keys=[\"video\", \"audio\"],\n",
    "    out_proj_dim=args.proj_dim,\n",
    "    num_att_heads=4,  # only for attention-based fusions\n",
    "    dropout=args.dropout,\n",
    ")\n",
    "\n",
    "if args.task == \"binary\":\n",
    "    detection_head = torch.nn.Sequential(\n",
    "        torch.nn.Linear(EMB_DIM, 1), torch.nn.Sigmoid()\n",
    "    )\n",
    "elif args.task == \"fine-grained\":\n",
    "    detection_head = torch.nn.Sequential(\n",
    "        torch.nn.Linear(EMB_DIM, 4), torch.nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "pipe = MultiModalAuthPipeline(\n",
    "    models=models,\n",
    "    fusion=fusion,\n",
    "    detection_head=detection_head,\n",
    "    freeze_backbone=True,\n",
    ")\n",
    "\n",
    "state_dict = torch.load(ckpt, map_location=\"cpu\")['state_dict']\n",
    "state_dict = {k.replace(\"pipeline.\", \"\"): v for k, v in state_dict.items()}\n",
    "pipe.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "pipe = pipe.cuda()\n",
    "pipe.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE EVAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: 1\n",
      "Pred: 1\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(test_loader))\n",
    "\n",
    "# Place on GPU\n",
    "sample[\"video\"] = sample[\"video\"].squeeze(0).cuda()\n",
    "sample[\"audio\"] = sample[\"audio\"].squeeze(0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = pipe(sample)\n",
    "    \n",
    "    preds = (out[\"logits\"] > 0.5).type(torch.int64).cpu()\n",
    "    final_pred = torch.mode(preds, dim=0).values # NOTE: Majority vote\n",
    "    \n",
    "    print(\"GT:\", sample['metadata'][\"label\"].cpu().item())\n",
    "    print(\"Pred:\", final_pred.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = F1Score(task=\"binary\")\n",
    "acc = Accuracy(task=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4121894042554420ba9945a14d4c26c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1:   0.995\n",
      "Train Acc:  0.995\n"
     ]
    }
   ],
   "source": [
    "f1.reset()\n",
    "acc.reset()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "for sample in tqdm(train_loader):\n",
    "    sample[\"video\"] = sample[\"video\"].squeeze(0).cuda()\n",
    "    sample[\"audio\"] = sample[\"audio\"].squeeze(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = pipe(sample)\n",
    "        \n",
    "        preds = (out[\"logits\"] > 0.5).type(torch.int64).cpu()\n",
    "        final_pred = torch.mode(preds, dim=0).values\n",
    "        gt = sample['metadata'][\"label\"].cpu()\n",
    "        \n",
    "        f1.update(final_pred, gt)\n",
    "        acc.update(final_pred, gt)\n",
    "        \n",
    "print(f\"Train F1:  {f1.compute().item(): .3f}\")\n",
    "print(f\"Train Acc: {acc.compute().item(): .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cdc3b2150d4083ba223668a070bf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1:   0.981\n",
      "Dev Acc:  0.981\n"
     ]
    }
   ],
   "source": [
    "f1.reset()\n",
    "acc.reset()\n",
    "\n",
    "dev_loader = dm.val_dataloader()\n",
    "for sample in tqdm(dev_loader):\n",
    "    sample[\"video\"] = sample[\"video\"].squeeze(0).cuda()\n",
    "    sample[\"audio\"] = sample[\"audio\"].squeeze(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = pipe(sample)\n",
    "        \n",
    "        preds = (out[\"logits\"] > 0.5).type(torch.int64).cpu()\n",
    "        final_pred = torch.mode(preds, dim=0).values\n",
    "        gt = sample['metadata'][\"label\"].cpu()\n",
    "        \n",
    "        f1.update(final_pred, gt)\n",
    "        acc.update(final_pred, gt)\n",
    "\n",
    "print(f\"Dev F1:  {f1.compute().item(): .3f}\")\n",
    "print(f\"Dev Acc: {acc.compute().item(): .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a900b568971a4c329674c202f8887095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1:   0.732\n",
      "Test Acc:  0.665\n"
     ]
    }
   ],
   "source": [
    "f1.reset()\n",
    "acc.reset()\n",
    "\n",
    "test_loader = dm.test_dataloader()\n",
    "for sample in tqdm(test_loader):\n",
    "    sample[\"video\"] = sample[\"video\"].squeeze(0).cuda()\n",
    "    sample[\"audio\"] = sample[\"audio\"].squeeze(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = pipe(sample)\n",
    "        \n",
    "        preds = (out[\"logits\"] > 0.5).type(torch.int64).cpu()\n",
    "        final_pred = torch.mode(preds, dim=0).values\n",
    "        gt = sample['metadata'][\"label\"].cpu()\n",
    "        \n",
    "        f1.update(final_pred, gt)\n",
    "        acc.update(final_pred, gt)\n",
    "        \n",
    "print(f\"Test F1:  {f1.compute().item(): .3f}\")\n",
    "print(f\"Test Acc: {acc.compute().item(): .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[\"video_ref\"] = torch.rand_like(out[\"video\"])\n",
    "# out[\"audio_ref\"] = torch.rand_like(out[\"audio\"])\n",
    "\n",
    "# # out['video_ref'] = out['video'].clone()\n",
    "# # out['audio_ref'] = out['audio'].clone()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     sim = pipe.verify(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.457475662231445, 1.972063422203064)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sim[\"video\"][0].item(), sim[\"audio\"][0].item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
