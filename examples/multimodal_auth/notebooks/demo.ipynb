{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.pipe import MultiModalAuthPipeline, ImagePreprocessor, AudioPreprocessor, AdaFace, ReDimNet, ClassifierHead\n",
    "import json\n",
    "from pathlib import Path\n",
    "from synthweave.utils.fusion import get_fusion\n",
    "from synthweave.utils.tools import read_video\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_module_dir = Path(\"/home/woleek/SynthWeave/models/CAFF\")\n",
    "\n",
    "# config\n",
    "args = json.loads((fusion_module_dir / \"args.json\").read_text())\n",
    "args = dotdict(args)\n",
    "\n",
    "# weights\n",
    "weights_path = fusion_module_dir / \"detection_module.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/woleek/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "preprocessors = {\n",
    "    \"video\": ImagePreprocessor(\n",
    "        window_len=4,\n",
    "        step=1,\n",
    "        estimate_quality=False,\n",
    "        models_dir=\"/home/woleek/SynthWeave/models\",\n",
    "        quality_model_type=\"ir50\"\n",
    "    ),\n",
    "    \"audio\": AudioPreprocessor(\n",
    "        window_len=4,\n",
    "        step=1,\n",
    "        use_vad=True,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/woleek/.cache/torch/hub/IDRnD_ReDimNet_master\n",
      "load_res : <All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/woleek/.cache/torch/hub/IDRnD_ReDimNet_master\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"video\": AdaFace(\n",
    "        path=\"/home/woleek/SynthWeave/models\",\n",
    "        model_type=\"ir50\",\n",
    "    ), \n",
    "    \"audio\": ReDimNet()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] This fusion expects embeddings of shape (batch_size, embed_dim).\n"
     ]
    }
   ],
   "source": [
    "fusion = get_fusion(\n",
    "    fusion_name=args.fusion,\n",
    "    output_dim=args.emb_dim,\n",
    "    modality_keys=[\"video\", \"audio\"],\n",
    "    input_dims={\"video\":512, \"audio\":192},\n",
    "    out_proj_dim=args.proj_dim,\n",
    ")\n",
    "\n",
    "detection_head = ClassifierHead(input_dim=args.emb_dim, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = MultiModalAuthPipeline(\n",
    "    processors=preprocessors,\n",
    "    models=models,\n",
    "    fusion=fusion,\n",
    "    detection_head=detection_head,\n",
    "    freeze_backbone=True,\n",
    "    iil_mode=args.iil_mode,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")\n",
    "\n",
    "state_dict = torch.load(weights_path, map_location=\"cpu\")['state_dict']\n",
    "state_dict = {k.replace(\"pipeline.\", \"\"): v for k, v in state_dict.items()}\n",
    "pipe.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "pipe = pipe.cuda()\n",
    "pipe.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = Path(\"../demo/samples/john_face_fake.mp4\").resolve()\n",
    "bf_sample = Path(\"../demo/samples/john_real.mp4\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid, aud, meta = read_video(df_sample)\n",
    "df = {\n",
    "    \"video\": [vid, meta['video_fps']],\n",
    "    \"audio\": [aud, meta['audio_fps']],\n",
    "    \"metadata\": meta\n",
    "}\n",
    "\n",
    "vid, aud, meta = read_video(bf_sample)\n",
    "bf = {\n",
    "    \"video\": [vid, meta['video_fps']],\n",
    "    \"audio\": [aud, meta['audio_fps']],\n",
    "    \"metadata\": meta\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    df_out = pipe(df) # Sample for pipeline inference\n",
    "    bf_out = pipe(bf) # Simulates database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold based on quality, face/voice detection etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len:  4\n",
      "Processed len:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Original len: \", df_out['org_len'])\n",
    "print(\"Processed len: \", df_out['valid_len']) # NOTE: Can threshold based on % of dropped windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check DeepFake module prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: DeepFake\n"
     ]
    }
   ],
   "source": [
    "probs = torch.sigmoid(df_out[\"logits\"]).cpu()\n",
    "prob_per_clip = probs.mean()\n",
    "preds_per_clip = (prob_per_clip >= 0.5).long() # NOTE: set threshold for DeepFake detection\n",
    "\n",
    "print(\"Pred:\", \"Bonafide\" if preds_per_clip.item() == 0 else \"DeepFake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sim = pipe.verify({\n",
    "        \"video\": df_out[\"video\"].mean(dim=0).unsqueeze(0).cpu(),\n",
    "        \"audio\": df_out[\"audio\"].mean(dim=0).unsqueeze(0).cpu(),\n",
    "        \"video_ref\": bf_out[\"video\"].mean(dim=0).unsqueeze(0).cpu(), # NOTE: put refference embeddings here\n",
    "        \"audio_ref\": bf_out[\"audio\"].mean(dim=0).unsqueeze(0).cpu()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6023023724555969"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim['video'].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face verified: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3009/2250123981.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vid_sim = torch.tensor(sim[\"video\"]).item()          # Mean similarity to reference\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Select aggregation methods across windows\n",
    "# vid_sim = sim[\"video\"].max(dim=1).values                # Max similarity to any reference\n",
    "vid_sim = torch.tensor(sim[\"video\"]).item()          # Mean similarity to reference\n",
    "\n",
    "vid_th = 60.0 # NOTE: set threshold for video similarity\n",
    "\n",
    "passed = vid_sim > vid_th\n",
    "print(\"Face verified:\", passed) # True if all windows passed the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice verified: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3009/1121158799.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  aud_sim = torch.tensor(sim[\"audio\"]).item()          # Mean similarity to reference\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Select aggregation methods across windows\n",
    "# aud_sim = sim[\"audio\"].max(dim=1).values                # Max similarity to any reference\n",
    "aud_sim = torch.tensor(sim[\"audio\"]).item()          # Mean similarity to reference\n",
    "\n",
    "aud_th = 60.0 # NOTE: set threshold for video similarity\n",
    "\n",
    "passed = aud_sim > aud_th\n",
    "print(\"Voice verified:\", passed) # True if all windows passed the threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
