{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "TASK = \"binary\"\n",
    "\n",
    "TRAIN_DATASETS = {\n",
    "    \"DeepSpeak_v1_1\": None,\n",
    "    \"SWAN_DF\": None,\n",
    "}\n",
    "\n",
    "# Find all results.json files\n",
    "RESULTS = {\n",
    "    ds: glob.glob(f\"../logs/{ds}/{TASK}/*/version_*/results/*/metrics.json\")\n",
    "    for ds in TRAIN_DATASETS.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_ds in TRAIN_DATASETS.keys():\n",
    "    # List to hold all rows\n",
    "    rows = []\n",
    "\n",
    "    # Process each file\n",
    "    for file_path in RESULTS[train_ds]:\n",
    "        eval_dataset = Path(file_path).parent.name  # Extract 'eval_dataset'\n",
    "        result_dir = Path(file_path).parent\n",
    "        version = result_dir.parent.parent.name  # Extract 'version_X'\n",
    "        fusion = result_dir.parent.parent.parent.name  # Extract 'fusion'\n",
    "\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Attach PNG paths\n",
    "        png_paths = {\n",
    "            # \"train_audio_proj\": str(result_dir / \"train_audio_proj.png\"),\n",
    "            # \"train_video_proj\": str(result_dir / \"train_video_proj.png\"),\n",
    "            # \"train_fused\": str(result_dir / \"train_fused.png\"),\n",
    "            \"val_audio_proj\": str(result_dir / \"val_audio_proj.png\"),\n",
    "            \"val_video_proj\": str(result_dir / \"val_video_proj.png\"),\n",
    "            \"val_fused\": str(result_dir / \"val_fused.png\"),\n",
    "            \"test_audio_proj\": str(result_dir / \"test_audio_proj.png\"),\n",
    "            \"test_video_proj\": str(result_dir / \"test_video_proj.png\"),\n",
    "            \"test_fused\": str(result_dir / \"test_fused.png\"),\n",
    "        }\n",
    "\n",
    "        # Flatten the structure\n",
    "        for split in [\"val\", \"test\"]:  # \"train\",\n",
    "            if split in data:\n",
    "                row = {\n",
    "                    \"fusion\": fusion,\n",
    "                    \"version\": version,\n",
    "                    \"split\": split,\n",
    "                    \"dataset\": eval_dataset,\n",
    "                }\n",
    "                row.update(data[split])\n",
    "                row.update({k: v for k, v in png_paths.items() if k.startswith(split)})\n",
    "                rows.append(row)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"[WARN] {train_ds} produced no valid splits - skipping\")\n",
    "        TRAIN_DATASETS[train_ds] = {}  # or whatever default makes sense\n",
    "        continue\n",
    "\n",
    "    df = df.groupby([\"dataset\"])\n",
    "\n",
    "    eval_dfs = {}\n",
    "    for name, group in df:\n",
    "        group = group.drop(columns=[\"dataset\"])\n",
    "        # Split DataFrame\n",
    "        # train = group[group[\"split\"] == \"train\"].drop(columns=[\"split\"])\n",
    "        val = group[group[\"split\"] == \"val\"].drop(columns=[\"split\"])\n",
    "        test = group[group[\"split\"] == \"test\"].drop(columns=[\"split\"])\n",
    "\n",
    "        # Sorting: Best EER first\n",
    "        # train = train.sort_values(by=\"auc\", ascending=False).reset_index(drop=True)\n",
    "        val = val.sort_values(by=\"auc\", ascending=False).reset_index(drop=True)\n",
    "        test = test.sort_values(by=\"auc\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        eval_dfs[name[0]] = {}\n",
    "\n",
    "        for split in [\"val\", \"test\"]:\n",
    "            split_df = (\n",
    "                group[group[\"split\"] == split]\n",
    "                .drop(columns=[\"split\"])\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # Separate into metrics and image paths\n",
    "            metric_cols = [\n",
    "                col\n",
    "                for col in split_df.columns\n",
    "                if col\n",
    "                not in [\n",
    "                    \"val_audio_proj\",\n",
    "                    \"val_video_proj\",\n",
    "                    \"val_fused\",\n",
    "                    \"test_audio_proj\",\n",
    "                    \"test_video_proj\",\n",
    "                    \"test_fused\",\n",
    "                ]\n",
    "            ]\n",
    "            image_cols = [col for col in split_df.columns if col not in metric_cols] + [\"version\"]\n",
    "\n",
    "            eval_dfs[name[0]][split] = {\n",
    "                \"metrics\": split_df[metric_cols],\n",
    "                \"images\": split_df[image_cols],\n",
    "            }\n",
    "\n",
    "    TRAIN_DATASETS[train_ds] = eval_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximize_metrics = [\"acc\", \"ap\", \"prec\", \"rec\", \"f1\", \"auc\"]\n",
    "minimize_metrics = [\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_best_per_column(s):\n",
    "    if s.name in maximize_metrics:\n",
    "        is_best = s == s.max()\n",
    "    elif s.name in minimize_metrics:\n",
    "        is_best = s == s.min()\n",
    "    else:\n",
    "        is_best = [False] * len(s)\n",
    "    return [\n",
    "        (\n",
    "            \"background-color: red\"\n",
    "            if v and (s.name == \"auc\" or s.name == \"ap\")\n",
    "            else \"background-color: green\" if v else \"\"\n",
    "        )\n",
    "        for v in is_best\n",
    "    ]\n",
    "\n",
    "\n",
    "# \"00\", \"01\", \"10\", \"11\" -> \"RA-RV\", \"RA-FV\", \"FA-RV\", \"FA-FV\"\n",
    "av_classes = [\"RA-RV\", \"RA-FV\", \"FA-RV\", \"FA-FV\"]\n",
    "\n",
    "\n",
    "def plot_embeddings(images_df: pd.DataFrame, split: str, version: str):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(\n",
    "        f\"{split.capitalize()} Embeddings (model {version})\", fontsize=16, x=0.45\n",
    "    )\n",
    "\n",
    "    mod_names = [\"Audio Proj\", \"Video Proj\", \"Fused\"]\n",
    "\n",
    "    # Look up the row with the matching version\n",
    "    row = images_df[images_df[\"version\"] == version]\n",
    "    if row.empty:\n",
    "        print(f\"No images found for version {version} in {split} split.\")\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    for i, mod in enumerate(mod_names):\n",
    "        img_col = f\"{split}_{mod.lower().replace(' ', '_')}\"\n",
    "        if img_col in row and isinstance(row[img_col], str) and Path(row[img_col]).exists():\n",
    "            try:\n",
    "                img = mpimg.imread(row[img_col])\n",
    "                axs[i].imshow(img)\n",
    "                axs[i].axis(\"off\")\n",
    "            except Exception:\n",
    "                axs[i].set_visible(False)\n",
    "        else:\n",
    "            axs[i].set_visible(False)\n",
    "\n",
    "    handles = [\n",
    "        mpatches.Patch(color=plt.cm.tab10(i), label=av_classes[i])\n",
    "        for i in range(len(av_classes))\n",
    "    ]\n",
    "    fig.legend(handles=handles, loc=\"center right\", title=\"AV Classes\", borderaxespad=5)\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_1 = list(TRAIN_DATASETS.keys())[0]\n",
    "print(f\"Models trained on {train_ds_1}.\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n\")\n",
    "\n",
    "eval_dfs = TRAIN_DATASETS[train_ds_1]\n",
    "\n",
    "for eval_name, eval_data in eval_dfs.items():\n",
    "    print(f\"Results on {eval_name}{' (cross-dataset)' if eval_name != train_ds_1 else ''}:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Sort test metrics by AUC\n",
    "    test_metrics = (\n",
    "        eval_data[\"test\"][\"metrics\"]\n",
    "        .sort_values(by=\"auc\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Align val_metrics by test AUC order using 'version' as key\n",
    "    sorted_versions = test_metrics[\"version\"].tolist()\n",
    "    val_metrics = eval_data[\"val\"][\"metrics\"]\n",
    "    val_metrics = val_metrics.set_index(\"version\").loc[sorted_versions].drop_duplicates().reset_index()\n",
    "    \n",
    "    # Get best version (highest test AUC)\n",
    "    best_version = test_metrics.loc[0, \"version\"]\n",
    "\n",
    "    val_images = eval_data[\"val\"][\"images\"]\n",
    "    test_images = eval_data[\"test\"][\"images\"]\n",
    "\n",
    "    print(\"- Val (sorted by test AUC):\")\n",
    "    display(val_metrics.style.apply(highlight_best_per_column, axis=0))\n",
    "    plot_embeddings(val_images, \"val\", best_version)\n",
    "\n",
    "    print(\"- Test:\")\n",
    "    display(test_metrics.style.apply(highlight_best_per_column, axis=0))\n",
    "    plot_embeddings(test_images, \"test\", best_version)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_2 = list(TRAIN_DATASETS.keys())[1]\n",
    "print(f\"Models trained on {train_ds_2}.\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n\")\n",
    "\n",
    "eval_dfs = TRAIN_DATASETS[train_ds_2]\n",
    "\n",
    "for eval_name, eval_data in eval_dfs.items():\n",
    "    print(f\"Results on {eval_name}{' (cross-dataset)' if eval_name != train_ds_2 else ''}:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Sort test metrics by AUC\n",
    "    test_metrics = (\n",
    "        eval_data[\"test\"][\"metrics\"]\n",
    "        .sort_values(by=\"auc\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Align val_metrics by test AUC order using 'version' as key\n",
    "    sorted_versions = test_metrics[\"version\"].tolist()\n",
    "    val_metrics = eval_data[\"val\"][\"metrics\"]\n",
    "    val_metrics = val_metrics.set_index(\"version\").loc[sorted_versions].drop_duplicates().reset_index()\n",
    "    \n",
    "    # Get best version (highest test AUC)\n",
    "    best_version = test_metrics.loc[0, \"version\"]\n",
    "\n",
    "    val_images = eval_data[\"val\"][\"images\"]\n",
    "    test_images = eval_data[\"test\"][\"images\"]\n",
    "\n",
    "    print(\"- Val (sorted by test AUC):\")\n",
    "    display(val_metrics.style.apply(highlight_best_per_column, axis=0))\n",
    "    plot_embeddings(val_images, \"val\", best_version)\n",
    "\n",
    "    print(\"- Test:\")\n",
    "    display(test_metrics.style.apply(highlight_best_per_column, axis=0))\n",
    "    plot_embeddings(test_images, \"test\", best_version)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
